{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9453833b-ac05-47f7-a5fb-398bfd41d745",
   "metadata": {},
   "source": [
    "# 시간적 연속성이 있다고 가정할 경우\n",
    "\n",
    "> 서버 시스템 로그 파일인 `UserVitalSign`, `UserLocationLog`, `UserGyro` 데이터를 데이터베이스에 저장하고 데이터 동기화 및 예측 시스템 구성을 위해 고려해야 할 요소들을 단계별제시\n",
    "\n",
    "## 1. 데이터베이스에 저장 시 수정할 열\n",
    "- **중복되는 시간 정보 제거**: 각 데이터셋에 `REGISTERDATE`, 'MODIFYDATE' 등 저장 시점 관련 열이 있다면, 통합된 하난의 저장 시간 컬럼으로 대체 가능\n",
    "- **일관된 사용자 코드**: `USERCODE`나 `USERID`가 일관되지 않을 경우, 통합 과정에서 이를 하나의 기준으로 통일하여 각 데이터셋 간 연결성을 강화\n",
    "- **위치 정보 통합**: `UserLocationLog`의 `LATITUDE`, `LONGITUDE`는 중요한 위치 정보이므로 `UserVitalSign`, `UserGyro`와 결합할 경우 사용자가 위치를 기반으로 행동 예측이나 낙상 예측에 활용 가능\n",
    "\n",
    "## 2. 측정 주기 차이에 따른 데이터 동기화\n",
    "- **샘플링 주기 맞추기**: 각 데이터셋의 주기가 다르다면, 가장 짧은 주기를 기준으로 데이터를 리샘플링하거나 보간하여 동기화 가능. 예를 들어, `UserGyro`의 주기가 가장 짧다면 이를 기준으로 다른 데이터셋을 보간\n",
    "- **타임 스탬프 기반 보간**: `VITALDATE`, `CHECKTIME` 등의 타임 스탬프를 기준으로 보간(interpolation)하여 일정한 주기(예: 초 단위)로 데이터를 맞춤\n",
    "- **데이터 결합 시의 시간 오차 허용**: 동기화 과정에서 약간의 시간 차이를 허용할 수 있는 범위를 지정하여 각 측정 주기 간 차이를 줄일 수 있음\n",
    "\n",
    "## 3. 측정 주기 및 레이블 없이 위험 예측 시스템 구성하기\n",
    "- **주기 맞춤으로 행동 패턴 분석**: 기존 연구와 다른 방식으로 레이블 없는 데이터를 사용하여 행동 패턴을 추론 가능. 예를 들어, `UserGyro`에서 X, Y, Z 축의 변화로 특정 움직임을 파악하고, `UserVitalSign`의 심박수나 체온 변화와 결합해 위험 예측 가능\n",
    "- **이상치 탐지를 통한 비정상 상황 예측**: 레이블 없이 위험 예측을 할 때 이상치 탐지 모델을 사용하여 비정상적인 행동(예: 급격한 움직임)이나 생체 신호 변화를 탐지하여 위험도를 예측 가능\n",
    "- **데이터 클러스터링 및 패턴 분석**: 군집화(clustering)를 사용하여 패턴을 분류하고, 이러한 군집의 특성에 따라 위험 수준을 예측 가능. 예를 들어, 낮은 활동 패턴과 높은 심박수 변화 패턴을 가진 클러스터가 위험 가능성을 높게 판단하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723940c-4f42-4f8b-93ce-4cca8025b3db",
   "metadata": {},
   "source": [
    "### 각 데이터셋의 작업일자(`WORKDATE`)와 작업자(`USERCODE`)별로 측정된 불연속 구간을 고려하여 평균 주기를 계산하고, 이 주기에 맞추어 리샘플링 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b8b036-3974-43bf-9ecb-09bd58759fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "UserVitalSign 데이터 로드 완료.\n",
      "UserLocationLog 데이터 로드 완료.\n",
      "UserGyro 데이터 로드 완료.\n",
      "\n",
      "데이터 샘플 확인:\n",
      "UserVitalSign 데이터:\n",
      "        NO  WORKDATE  USERCODE  ISWEAR  HEARTBEAT  TEMPERATURE  \\\n",
      "0  1722304  20240801        -1       0          0          0.0   \n",
      "1  1722305  20240801        -1       0          0          0.0   \n",
      "2  1722306  20240801        -1       0          0          0.0   \n",
      "3  1722307  20240801        -1       0          0          0.0   \n",
      "4  1722308  20240801        -1       0          0          0.0   \n",
      "\n",
      "   OUTSIDETEMPERATURE  LATITUDE  LONGITUDE  DEVICEBATTERY  \\\n",
      "0                   0       0.0        0.0            100   \n",
      "1                   0       0.0        0.0            100   \n",
      "2                   0       0.0        0.0             -1   \n",
      "3                   0       0.0        0.0            100   \n",
      "4                   0       0.0        0.0            100   \n",
      "\n",
      "                VITALDATE             REGISTERDATE               MODIFYDATE  \\\n",
      "0 2024-08-01 01:17:28.823  2024-08-01 01:17:28.579  2024-08-01 01:17:28.579   \n",
      "1 2024-08-01 02:28:02.210  2024-08-01 02:28:02.166  2024-08-01 02:28:02.166   \n",
      "2 2024-08-01 03:00:47.245  2024-08-01 03:00:49.697  2024-08-01 03:00:49.697   \n",
      "3 2024-08-01 04:36:03.777  2024-08-01 04:36:04.699  2024-08-01 04:36:04.699   \n",
      "4 2024-08-01 05:31:22.964  2024-08-01 05:31:23.907  2024-08-01 05:31:23.907   \n",
      "\n",
      "   HEARTBEAT_OUTLIER  TEMPERATURE_OUTLIER  \n",
      "0              False                False  \n",
      "1              False                False  \n",
      "2              False                False  \n",
      "3              False                False  \n",
      "4              False                False  \n",
      "UserLocationLog 데이터:\n",
      "        NO  WORKDATE  USERCODE   LATITUDE   LONGITUDE  SATELLITE  ACCURACY  \\\n",
      "0  9254323  20240731        17  35.166052  129.056285          0      20.0   \n",
      "1  9254324  20240731        17  35.166052  129.056286          0      20.0   \n",
      "2  9254325  20240731        17  35.166052  129.056286          0      20.0   \n",
      "3  9254326  20240731        17  35.166052  129.056286          0      20.0   \n",
      "4  9254327  20240731        17  35.166052  129.056286          0      20.0   \n",
      "\n",
      "    ALTITUDE  VERTICALACCURACY  BEARING  BEARINGACCURACY  DIRECTION     SPEED  \\\n",
      "0  48.299999           1.00649      0.0              0.0          0  0.006681   \n",
      "1  48.299999           1.00571      0.0              0.0          0  0.003505   \n",
      "2  48.299999           1.00262      0.0              0.0          0  0.000786   \n",
      "3  48.299999           1.00868      0.0              0.0          0  0.000930   \n",
      "4  48.299999           1.01297      0.0              0.0          0  0.003900   \n",
      "\n",
      "   SPEEDACCURACY       UNIXTIME           CHECKTIME LOCATIONPROVIDER  \\\n",
      "0            1.5  1722351819421 2024-07-31 00:03:39            fused   \n",
      "1            1.5  1722351824414 2024-07-31 00:03:44            fused   \n",
      "2            1.5  1722351829509 2024-07-31 00:03:49            fused   \n",
      "3            1.5  1722351834709 2024-07-31 00:03:54            fused   \n",
      "4            1.5  1722351839659 2024-07-31 00:03:59            fused   \n",
      "\n",
      "   ELAPSEDREALTIMEAGEMILLIS                REGISTERDATE  \\\n",
      "0                       100  2024-08-01 07:24:56.185843   \n",
      "1                        85  2024-08-01 07:24:56.185843   \n",
      "2                       113  2024-08-01 07:24:56.185843   \n",
      "3                        82  2024-08-01 07:24:56.185843   \n",
      "4                        91  2024-08-01 07:24:56.185843   \n",
      "\n",
      "                   MODIFYDATE  \n",
      "0  2024-08-01 07:24:56.185843  \n",
      "1  2024-08-01 07:24:56.185843  \n",
      "2  2024-08-01 07:24:56.185843  \n",
      "3  2024-08-01 07:24:56.185843  \n",
      "4  2024-08-01 07:24:56.185843  \n",
      "UserGyro 데이터:\n",
      "         NO  WORKDATE  USERCODE         X         Y         Z  \\\n",
      "0  54456642  20240730        46 -0.290772 -0.069639 -0.182038   \n",
      "1  54456643  20240730        46  0.337198  0.179594  0.147829   \n",
      "2  54456644  20240730        46 -0.190590  0.024435  0.332311   \n",
      "3  54456645  20240730        46  0.161268  0.032987  0.453262   \n",
      "4  54456646  20240730        46  0.118508  0.097738  0.492357   \n",
      "\n",
      "                   VITALDATE                REGISTERDATE  \n",
      "0 2024-07-30 10:50:00.970684  2024-08-02 07:24:24.260952  \n",
      "1 2024-07-30 10:50:01.130875  2024-08-02 07:24:24.260952  \n",
      "2 2024-07-30 10:50:01.289972  2024-08-02 07:24:24.260952  \n",
      "3 2024-07-30 10:50:01.451762  2024-08-02 07:24:24.260952  \n",
      "4 2024-07-30 10:50:01.625562  2024-08-02 07:24:24.260952  \n",
      "1단계: 각 데이터셋의 그룹별 평균 주기 계산\n",
      "\n",
      "[VITALDATE] 그룹별 평균 주기 계산 중....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "불연속 구간 계산 진행: 100%|████████████████████████████████████████████████████████| 784/784 [00:00<00:00, 918.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VITALDATE] 평균 주기 계산 완료!\n",
      "\n",
      "\n",
      "[CHECKTIME] 그룹별 평균 주기 계산 중....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "불연속 구간 계산 진행: 100%|████████████████████████████████████████████████████████| 949/949 [00:01<00:00, 561.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECKTIME] 평균 주기 계산 완료!\n",
      "\n",
      "\n",
      "[VITALDATE] 그룹별 평균 주기 계산 중....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "불연속 구간 계산 진행: 100%|█████████████████████████████████████████████████████████| 472/472 [00:05<00:00, 80.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VITALDATE] 평균 주기 계산 완료!\n",
      "\n",
      "\n",
      "UserVitalSign 평균 주기 데이터:\n",
      "        NO  WORKDATE  USERCODE  ISWEAR  HEARTBEAT  TEMPERATURE  \\\n",
      "0  1722304  20240801        -1       0          0     0.000000   \n",
      "1  1789774  20240801         1       0         -1    -1.000000   \n",
      "2  1723961  20240801        13       1         79    29.634054   \n",
      "3  1724316  20240801        14       1         83    31.905823   \n",
      "4  1725375  20240801        15       1          0     0.000000   \n",
      "\n",
      "   OUTSIDETEMPERATURE   LATITUDE   LONGITUDE  DEVICEBATTERY  \\\n",
      "0                   0   0.000000    0.000000            100   \n",
      "1                  -1   0.000000    0.000000             53   \n",
      "2                  30  35.166053  129.056287            100   \n",
      "3                  31  35.165800  129.056021            100   \n",
      "4                  -1  35.166056  129.056285            100   \n",
      "\n",
      "                VITALDATE             REGISTERDATE               MODIFYDATE  \\\n",
      "0 2024-08-01 01:17:28.823  2024-08-01 01:17:28.579  2024-08-01 01:17:28.579   \n",
      "1 2024-08-01 12:59:31.595  2024-08-01 20:59:30.837  2024-08-01 20:59:30.837   \n",
      "2 2024-08-01 07:19:40.507  2024-08-01 07:19:39.853  2024-08-01 07:19:39.853   \n",
      "3 2024-08-01 07:21:24.848  2024-08-01 07:21:21.666  2024-08-01 07:21:21.666   \n",
      "4 2024-08-01 07:25:50.235  2024-08-01 07:25:49.861  2024-08-01 07:25:49.861   \n",
      "\n",
      "   HEARTBEAT_OUTLIER  TEMPERATURE_OUTLIER  AVG_INTERVAL  \n",
      "0              False                False   1385.926063  \n",
      "1              False                False    114.817500  \n",
      "2              False                False     17.583915  \n",
      "3              False                False     18.764262  \n",
      "4              False                False     19.055586  \n",
      "\n",
      "UserLocationLog 평균 주기 데이터:\n",
      "        NO  WORKDATE  USERCODE   LATITUDE   LONGITUDE  SATELLITE  ACCURACY  \\\n",
      "0  9247814  20240731        16  35.166056  129.056286          0    20.000   \n",
      "1  9254323  20240731        17  35.166052  129.056285          0    20.000   \n",
      "2  9248282  20240731        23  35.167184  129.056179          0    14.573   \n",
      "3  9267432  20240731        26  35.169775  129.057421          0     9.500   \n",
      "4  9626310  20240801         1  35.174422  129.128035          0    12.839   \n",
      "\n",
      "    ALTITUDE  VERTICALACCURACY  BEARING  ...  DIRECTION     SPEED  \\\n",
      "0  48.299999          1.781340    0.000  ...          0  0.000791   \n",
      "1  48.299999          1.006490    0.000  ...          0  0.006681   \n",
      "2  56.599998          0.802246  205.441  ...          0  0.412191   \n",
      "3  62.076015          7.629690  317.632  ...          0  0.906415   \n",
      "4  62.299999          1.996510    0.000  ...          0  0.000000   \n",
      "\n",
      "   SPEEDACCURACY       UNIXTIME           CHECKTIME LOCATIONPROVIDER  \\\n",
      "0           1.50  1722437961611 2024-07-31 23:59:21            fused   \n",
      "1           1.50  1722351819421 2024-07-31 00:03:39            fused   \n",
      "2           1.50  1722389234977 2024-07-31 10:27:15            fused   \n",
      "3           0.42  1722390004777 2024-07-31 10:40:19            fused   \n",
      "4           1.50  1722496655618 2024-08-01 16:17:35            fused   \n",
      "\n",
      "  ELAPSEDREALTIMEAGEMILLIS                REGISTERDATE  \\\n",
      "0                      110  2024-08-01 07:12:43.713428   \n",
      "1                      100  2024-08-01 07:24:56.185843   \n",
      "2                      194  2024-08-01 07:15:13.040631   \n",
      "3                    14643  2024-08-01 07:44:52.553925   \n",
      "4                      309  2024-08-01 16:17:45.754247   \n",
      "\n",
      "                   MODIFYDATE AVG_INTERVAL  \n",
      "0  2024-08-01 07:12:43.713428     5.142857  \n",
      "1  2024-08-01 07:24:56.185843     5.000000  \n",
      "2  2024-08-01 07:15:13.040631   324.919463  \n",
      "3  2024-08-01 07:44:52.553925     6.666667  \n",
      "4  2024-08-01 16:17:45.754247    73.083333  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "UserGyro 평균 주기 데이터:\n",
      "         NO  WORKDATE  USERCODE         X         Y         Z  \\\n",
      "0  54456642  20240730        46 -0.290772 -0.069639 -0.182038   \n",
      "1  54235309  20240731        37 -0.147829 -0.219911  0.199142   \n",
      "2  54378177  20240731        40 -0.006109 -0.024435 -0.006109   \n",
      "3  53688207  20240731        47  0.040317  0.018326  0.012217   \n",
      "4  54337112  20240801         1  0.021991 -0.010996 -0.003665   \n",
      "\n",
      "                   VITALDATE                REGISTERDATE  AVG_INTERVAL  \n",
      "0 2024-07-30 10:50:00.970684  2024-08-02 07:24:24.260952      0.171769  \n",
      "1 2024-07-31 10:56:20.078409  2024-08-01 10:34:50.134123      0.162068  \n",
      "2 2024-07-31 10:55:57.622918  2024-08-02 07:12:00.830956      0.225805  \n",
      "3 2024-07-31 10:13:08.169981  2024-08-01 09:15:42.526845     17.971808  \n",
      "4 2024-08-01 16:17:38.776896  2024-08-01 16:17:59.257361      1.732558  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"데이터 로드 중...\")\n",
    "\n",
    "# UserVitalSign 데이터 로드\n",
    "vital_sign = pd.read_csv('Cleaned_UserVitalSign.csv', parse_dates=['VITALDATE'])\n",
    "print(\"UserVitalSign 데이터 로드 완료.\")\n",
    "\n",
    "# UserLocationLog 데이터 로드\n",
    "location_log = pd.read_csv('Cleaned_UserLocationLog.csv', parse_dates=['CHECKTIME'])\n",
    "print(\"UserLocationLog 데이터 로드 완료.\")\n",
    "\n",
    "# UserGyro 데이터 로드\n",
    "gyro_data = pd.read_csv('Cleaned_All_UserGyro.csv', parse_dates=['VITALDATE'])\n",
    "print(\"UserGyro 데이터 로드 완료.\")\n",
    "\n",
    "# 데이터가 로드되었는지 확인\n",
    "print(\"\\n데이터 샘플 확인:\")\n",
    "print(\"UserVitalSign 데이터:\", vital_sign.head(), sep=\"\\n\")\n",
    "print(\"UserLocationLog 데이터:\", location_log.head(), sep=\"\\n\")\n",
    "print(\"UserGyro 데이터:\", gyro_data.head(), sep=\"\\n\")\n",
    "\n",
    "# 1. 불연속 구간을 고려한 평균 주기 계산 함수 정의\n",
    "def calculate_avg_interval_with_discontinuity(df, date_column, threshold=3600):\n",
    "    print(f\"\\n[{date_column}] 그룹별 평균 주기 계산 중....\")\n",
    "    \n",
    "    avg_intervals = []\n",
    "    # WORKDATE와 USERCODE로 그룹화\n",
    "    grouped = df.groupby(['WORKDATE', 'USERCODE'])\n",
    "\n",
    "    for (workdate, usercode), group in tqdm(grouped, desc=\"불연속 구간 계산 진행\"):\n",
    "        group = group.sort_values(by=date_column)\n",
    "        \n",
    "        # VITALDATE 간 시간 차이 계산\n",
    "        time_diffs = group[date_column].diff().dt.total_seconds().dropna()\n",
    "\n",
    "        # 특정 시간 간격을 넘는 차이를 불연속 구간으로 간주하고 필터링\n",
    "        filtered_diffs = time_diffs[time_diffs <= threshold]\n",
    "\n",
    "        # 불연속 구간 제외 후 평균 주기 계산\n",
    "        avg_interval = filtered_diffs.mean() if not filtered_diffs.empty else None\n",
    "        \n",
    "        # 그룹의 첫 번째 행에서 필요한 열을 가져와 추가\n",
    "        additional_data = group.iloc[0].to_dict()\n",
    "        additional_data.update({'WORKDATE': workdate, 'USERCODE': usercode, 'AVG_INTERVAL': avg_interval})\n",
    "        avg_intervals.append(additional_data)\n",
    "\n",
    "    # 결과를 데이터프레임으로 반환\n",
    "    avg_interval_df = pd.DataFrame(avg_intervals)\n",
    "    print(f\"[{date_column}] 평균 주기 계산 완료!\\n\")\n",
    "    return avg_interval_df\n",
    "\n",
    "# 2. 각 데이터셋에 대해 불연속 구간을 고려한 평균 주기 계산\n",
    "print(\"1단계: 각 데이터셋의 그룹별 평균 주기 계산\")\n",
    "vital_avg_intervals = calculate_avg_interval_with_discontinuity(vital_sign, 'VITALDATE', threshold=3600)\n",
    "location_avg_intervals = calculate_avg_interval_with_discontinuity(location_log, 'CHECKTIME', threshold=3600)\n",
    "gyro_avg_intervals = calculate_avg_interval_with_discontinuity(gyro_data, 'VITALDATE', threshold=3600)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\nUserVitalSign 평균 주기 데이터:\")\n",
    "print(vital_avg_intervals.head())\n",
    "print(\"\\nUserLocationLog 평균 주기 데이터:\")\n",
    "print(location_avg_intervals.head())\n",
    "print(\"\\nUserGyro 평균 주기 데이터:\")\n",
    "print(gyro_avg_intervals.head())\n",
    "\n",
    "\n",
    "# # 3. 주기를 반영한 리샘플링 수행 함수 정의\n",
    "# def resample_by_avg_interval(df, date_column, avg_intervals_df):\n",
    "#     print(f\"\\n[{date_column}] 주기 기반 리샘플링 시작...\")\n",
    "#     resampled_dfs = []\n",
    "\n",
    "#     for (workdate, usercode), group in tqdm(df.groupby(['WORKDATE', 'USECODE']), desc=\"그룹별 리샘플링 진행\"):\n",
    "#         # 평균 주기 가져오기\n",
    "#         avg_interval = avg_intervals_df.loc[\n",
    "#             (avg_intervals_df['WORKDATE'] == workdate) &\n",
    "#             (avg_intervals_df['USERCODE'] == usercode), 'AVG_INTERVAL'\n",
    "#         ]\n",
    "\n",
    "#         if not avg_interval.empty and not np.isnan(avg_interval.values[0]):\n",
    "#             group = group.set_index(date_column)\n",
    "#             # 주기 평균에 맞추어 리샘플링\n",
    "#             resample_freq = f\"{int(avg_interval.values[0])}S\"\n",
    "#             resampled_group = group.resample(resample_freq).ffill().reset_index()\n",
    "#             resampled_group['WORKDATE'] = workdate\n",
    "#             resampled_group['USERCODE'] = usercode\n",
    "#             resampled_dfs.append(resampled_group)\n",
    "\n",
    "#     # 모든 리샘플링된 그룹 결합\n",
    "#     resampled_df = pd.concat(resamped_dfs, ignore_index=True)\n",
    "#     print(f\"[{date_column}] 리샘플링 완료!\\n\")\n",
    "#     return resampled_df\n",
    "    \n",
    "# # 4. 각 데이터셋 동기화\n",
    "# print(\"2단계: 각 데이터셋 주기 기반 리샘플링 수행\")\n",
    "# resampled_vital = resample_by_avg_interval(vital_sign, 'VITALDATE', vital_avg_intervals)\n",
    "# resampled_location = resample_by_avg_interval(location_log, 'CHECKTIME', location_avg_intervals)\n",
    "# resampled_gyro = resample_by_avg_interval(gyro, 'VITALDATE', gyro_avg_intervals)\n",
    "\n",
    "# # 5. 동기화된 데이터 확인\n",
    "# print(\"\\n동기화된 UserVitalSign 데이터:\")\n",
    "# print(resampled_vital.head())\n",
    "# print(\"\\n동기화된 UserLocationLog 데이터:\")\n",
    "# print(resampled_location.head())\n",
    "# print(\"\\n동기화된 UserGyro 데이터:\")\n",
    "# print(resampled_gyro.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76222dff-e171-432a-91e3-ac5ccf00e513",
   "metadata": {},
   "source": [
    "# 이벤트 발생 기반의 로그 데이터의 특성과 작업일자 및 작업자별 통합을 고려하여, 실시간 위험 예측 웹 서비스를 구현하기 위해서는 각 데이터셋의 발생 시점을 기준으로 동기화해야 함\n",
    "\n",
    "## 1. 데이터셋 기본 구조 설정\n",
    "- **데이터셋의 특성 파악**: `UserGyro`, `UserVitalSign`, `UserLocationLog`는 모두 변화가 감지될 때만 기록되는 이벤트 기반 로그\n",
    "- **통합 기준**: 발생 시점(`VITALDATE`, `CHECKTIME`)을 기준으로 작업자별(`USERCODE`)과 작업일자별(`WORKDATE`)로 그룹화하여 동기화하는 것이 필요\n",
    "\n",
    "## 2. 데이터 전처리 및 그룹화\n",
    "1. **불필요한 열 제거**: `NO` 열은 단순히 로그 순서 키이므로 제거\n",
    "2. **작업일자와 작업자별로 그룹화**: `WORKDATE`와 `USERCODE` 기준으로 그룹화하여 각각의 데이터를 다룸\n",
    "3. **이벤트 간격 분석**: 연속성 여부를 파악하기 위해 이벤트 간 시간 간격을 계산. 연속적 간격은 그대로 두고, 불연속적인 간격의 경우는 특정 기준으로 간격을 조정하거나 비어있는 데이터를 보완할지 결정\n",
    "\n",
    "## 3. 데이터 동기화\n",
    "- **기준 시간 설정**: 위험 예측 모델에서 중요하게 다루는 이벤트 발생 시간을 기준으로 다른 데이터셋의 시계열 데이터를 보간하거나 맞춤\n",
    "    - 예: UserGyro의 행동 변화 시점을 중심으로, 동일한 `VITALDATE` 또는 근접 시간(`±tolerance`초 이내)에 발생한 심박수, 위치 등의 데이터가 포함되도록 맞춤\n",
    "- **시간적 불연속 구간 처리**: 불연속적인 구간이 발생하는 경우, 해당 구간에 대한 위험 예측 분석에서의 중요성을 고려하여 간격이 큰 경우는 비어 있는 데이터로 두거나, 중간값 보간으로 데이터를 보완 가능\n",
    "\n",
    "## 4. 실시간 분석 및 모델 적용\n",
    "- **이벤트 발생 시점 기준 윈도우 생성**: 이벤트 기반 로그이므로 슬라이딩 윈도우를 설정하여 각 이벤트가 발생한 시점을 중심으로 윈도우를 설정함\n",
    "- **모델 훈련 및 예측**: 훈련된 위험 예측 모델을 이벤트 발생 시점별로 분석하여, 실시간으로 위험 여부를 판단함. 이때 예측된 행동 변화와 생체 신호, 위치 데이터를 조합하여 위험 수준을 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f97829-bd9b-4cc4-ba21-eb74d383a90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-26 23:44:25,995 [INFO] 데이터 로드 중...\n",
      "2024-10-26 23:45:32,189 [INFO] 데이터 로드 완료!\n",
      "2024-10-26 23:45:32,190 [INFO] UserVitalSign 데이터 동기화 시작...\n",
      "2024-10-26 23:45:48,492 [INFO] [UserVitalSign] 동기화 후 누락 데이터 개수: 11300440\n",
      "2024-10-26 23:46:03,258 [INFO] [UserVitalSign] ffill 처리 후 누락 데이터 개수: 14740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6652\\3829536318.py:96: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df = merged_df.bfill()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-26 23:46:14,783 [INFO] [UserVitalSign] bfill 처리 후 누락 데이터 개수: 0\n",
      "2024-10-26 23:46:14,784 [INFO] UserVitalSign 데이터 동기화 완료.\n",
      "2024-10-26 23:46:14,889 [INFO] UserLocationLog 데이터 동기화 시작...\n",
      "2024-10-26 23:46:39,339 [INFO] [UserLocationLog] 동기화 후 누락 데이터 개수: 1864915\n",
      "2024-10-26 23:46:52,448 [INFO] [UserLocationLog] ffill 처리 후 누락 데이터 개수: 19162\n",
      "2024-10-26 23:47:05,133 [INFO] [UserLocationLog] bfill 처리 후 누락 데이터 개수: 0\n",
      "2024-10-26 23:47:05,134 [INFO] UserLocationLog 데이터 동기화 완료.\n",
      "2024-10-26 23:47:06,205 [INFO] 불필요한 열 정리 완료. 최종 데이터 구조:\n",
      "2024-10-26 23:47:06,206 [INFO]             VITALDATE_synced    CHECKTIME_synced  HEARTBEAT  TEMPERATURE  \\\n",
      "0 2024-07-30 10:50:00.970684 2024-08-01 08:11:50       95.0    36.015884   \n",
      "1 2024-07-30 10:50:01.130875 2024-08-01 08:11:50       95.0    36.015884   \n",
      "2 2024-07-30 10:50:01.289972 2024-08-01 08:11:50       95.0    36.015884   \n",
      "3 2024-07-30 10:50:01.451762 2024-08-01 08:11:50       95.0    36.015884   \n",
      "4 2024-07-30 10:50:01.625562 2024-08-01 08:11:50       95.0    36.015884   \n",
      "\n",
      "   OUTSIDETEMPERATURE   LATITUDE   LONGITUDE    SPEED         X         Y  \\\n",
      "0                35.0  35.165512  129.057419  1.23229 -0.290772 -0.069639   \n",
      "1                35.0  35.165512  129.057419  1.23229  0.337198  0.179594   \n",
      "2                35.0  35.165512  129.057419  1.23229 -0.190590  0.024435   \n",
      "3                35.0  35.165512  129.057419  1.23229  0.161268  0.032987   \n",
      "4                35.0  35.165512  129.057419  1.23229  0.118508  0.097738   \n",
      "\n",
      "          Z  \n",
      "0 -0.182038  \n",
      "1  0.147829  \n",
      "2  0.332311  \n",
      "3  0.453262  \n",
      "4  0.492357  \n",
      "2024-10-26 23:51:46,390 [INFO] 최종 동기화된 데이터를 'Synchronized_All_Data.csv'에 저장하였습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# tolerance 시간 설정 (5초 이내의 시간으로 동기화)\n",
    "TOLERANCE_SECONDS = 5\n",
    "tolerance = timedelta(seconds=TOLERANCE_SECONDS)\n",
    "\n",
    "def load_data(gyro_path, vital_path, location_path):\n",
    "    \"\"\"\n",
    "    데이터 로드 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"데이터 로드 중...\")\n",
    "        gyro_data = pd.read_csv(gyro_path, parse_dates=['VITALDATE'])\n",
    "        vital_data = pd.read_csv(vital_path, parse_dates=['VITALDATE'])\n",
    "        location_data = pd.read_csv(location_path, parse_dates=['CHECKTIME'])\n",
    "        logger.info(\"데이터 로드 완료!\")\n",
    "        return gyro_data, vital_data, location_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"데이터 로드 중 오류 발생: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def synchronize_event_based_data(main_df, secondary_df, main_time_col, secondary_time_col, desc=\"\"):\n",
    "    \"\"\"\n",
    "    이벤트 기반 데이터 동기화 함수.\n",
    "    \"\"\"\n",
    "    logger.info(f\"{desc} 데이터 동기화 시작...\")\n",
    "    try:\n",
    "        # 불필요한 열 제거\n",
    "        for col in ['NO', 'USERCODE', 'WORKDATE']:\n",
    "            if col in main_df.columns:\n",
    "                main_df = main_df.drop(columns=[col])\n",
    "            if col in secondary_df.columns:\n",
    "                secondary_df = secondary_df.drop(columns=[col])\n",
    "\n",
    "        # 시간 열을 datetime 타입으로 변환\n",
    "        secondary_df[secondary_time_col] = pd.to_datetime(secondary_df[secondary_time_col], errors='coerce')\n",
    "        main_df[main_time_col] = pd.to_datetime(main_df[main_time_col], errors='coerce')\n",
    "\n",
    "        # 시간 열에 결측치가 있는 행 제거\n",
    "        main_df = main_df.dropna(subset=[main_time_col])\n",
    "        secondary_df = secondary_df.dropna(subset=[secondary_time_col])\n",
    "\n",
    "        # 데이터 타입 다운캐스팅 (메모리 최적화)\n",
    "        for df in [main_df, secondary_df]:\n",
    "            for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float' if df[col].dtype == 'float64' else 'integer')\n",
    "\n",
    "        # 병합 전에 secondary_df에서 중복 열 제거 (시간 열 제외)\n",
    "        common_cols = set(main_df.columns).intersection(set(secondary_df.columns))\n",
    "        cols_to_drop = common_cols - {secondary_time_col}\n",
    "        if cols_to_drop:\n",
    "            secondary_df = secondary_df.drop(columns=cols_to_drop)\n",
    "\n",
    "        # 병합 (USERCODE 및 WORKDATE 없이)\n",
    "        merged_df = pd.merge_asof(\n",
    "            main_df.sort_values(main_time_col),\n",
    "            secondary_df.sort_values(secondary_time_col),\n",
    "            left_on=main_time_col,\n",
    "            right_on=secondary_time_col,\n",
    "            direction=\"nearest\",\n",
    "            tolerance=tolerance\n",
    "        )\n",
    "\n",
    "        # 동기화된 시간 열 이름 변경\n",
    "        merged_df = merged_df.rename(columns={secondary_time_col: f\"{secondary_time_col}_synced\"})\n",
    "\n",
    "        # 병합으로 생성된 _x, _y 접미사 열 제거\n",
    "        suffixes = ['_x', '_y']\n",
    "        cols_to_drop = [col for col in merged_df.columns if any(col.endswith(suffix) for suffix in suffixes)]\n",
    "        if cols_to_drop:\n",
    "            merged_df = merged_df.drop(columns=cols_to_drop)\n",
    "\n",
    "        # 누락 데이터 확인\n",
    "        missing_count = merged_df.isnull().sum().sum()\n",
    "        logger.info(f\"[{desc}] 동기화 후 누락 데이터 개수: {missing_count}\")\n",
    "\n",
    "        # 결측치 처리 (전체 데이터프레임에 대해)\n",
    "        merged_df = merged_df.ffill()\n",
    "        missing_count = merged_df.isnull().sum().sum()\n",
    "        logger.info(f\"[{desc}] ffill 처리 후 누락 데이터 개수: {missing_count}\")\n",
    "\n",
    "        if missing_count > 0:\n",
    "            merged_df = merged_df.bfill()\n",
    "            missing_count = merged_df.isnull().sum().sum()\n",
    "            logger.info(f\"[{desc}] bfill 처리 후 누락 데이터 개수: {missing_count}\")\n",
    "\n",
    "            if missing_count > 0:\n",
    "                merged_df = merged_df.fillna('NaN')\n",
    "                logger.info(f\"[{desc}] 남은 누락 데이터 'NaN'으로 대체 완료\")\n",
    "\n",
    "        logger.info(f\"{desc} 데이터 동기화 완료.\")\n",
    "        return merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{desc} 데이터 동기화 중 오류 발생: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def clean_final_columns(df):\n",
    "    \"\"\"\n",
    "    최종 분석에 필요한 열만 유지하고 나머지 삭제하는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 최종 분석에 필요한 열 리스트\n",
    "        keep_columns = [\n",
    "            'VITALDATE', 'VITALDATE_synced', 'CHECKTIME', 'CHECKTIME_synced',\n",
    "            # 분석에 필요한 열 추가\n",
    "            'HEARTBEAT', 'TEMPERATURE', 'OUTSIDETEMPERATURE',\n",
    "            'LATITUDE', 'LONGITUDE', 'SPEED', 'X', 'Y', 'Z'\n",
    "        ]\n",
    "        # 존재하지 않는 열은 무시하고 유지\n",
    "        keep_columns = [col for col in keep_columns if col in df.columns]\n",
    "        df = df[keep_columns]\n",
    "        logger.info(\"불필요한 열 정리 완료. 최종 데이터 구조:\")\n",
    "        logger.info(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"최종 열 정리 중 오류 발생: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def save_data(df, output_path):\n",
    "    \"\"\"\n",
    "    최종 데이터를 저장하는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"최종 동기화된 데이터를 '{output_path}'에 저장하였습니다.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"데이터 저장 중 오류 발생: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    # 데이터 경로 설정\n",
    "    gyro_path = 'Cleaned_All_UserGyro.csv'\n",
    "    vital_path = 'Cleaned_UserVitalSign.csv'\n",
    "    location_path = 'Cleaned_UserLocationLog.csv'\n",
    "    output_path = 'Synchronized_All_Data.csv'\n",
    "\n",
    "    # 데이터 로드\n",
    "    gyro_data, vital_data, location_data = load_data(gyro_path, vital_path, location_path)\n",
    "\n",
    "    # 1단계: UserVitalSign 데이터 동기화\n",
    "    synchronized_data = synchronize_event_based_data(\n",
    "        gyro_data, vital_data, 'VITALDATE', 'VITALDATE', desc=\"UserVitalSign\"\n",
    "    )\n",
    "\n",
    "    # 2단계: UserLocationLog 데이터 동기화\n",
    "    synchronized_data = synchronize_event_based_data(\n",
    "        synchronized_data, location_data, 'VITALDATE_synced', 'CHECKTIME', desc=\"UserLocationLog\"\n",
    "    )\n",
    "\n",
    "    # 3단계: 최종 데이터 정리\n",
    "    synchronized_data = clean_final_columns(synchronized_data)\n",
    "\n",
    "    # 최종 데이터 저장\n",
    "    save_data(synchronized_data, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285c9a3-d139-467d-9b96-1bfba351ef07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
